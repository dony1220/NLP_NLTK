{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Fine-tune a pretrained model :  \n",
    "## NSMC 감성분석 데이터 학습 및 분류기 만들기  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Downloading numpy-1.26.3-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.2/61.2 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Downloading pyarrow-14.0.2-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.1.4-cp310-cp310-win_amd64.whl.metadata (18 kB)\n",
      "Collecting requests>=2.19.0 (from datasets)\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.62.1 (from datasets)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB ? eta 0:00:00\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.1-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from datasets) (23.2)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl.metadata (32 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.19.0->datasets)\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.19.0->datasets)\n",
      "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->datasets)\n",
      "  Downloading urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.19.0->datasets)\n",
      "  Downloading certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas->datasets)\n",
      "  Downloading tzdata-2023.4-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "   ---------------------------------------- 0.0/507.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 507.1/507.1 kB ? eta 0:00:00\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 115.3/115.3 kB 6.6 MB/s eta 0:00:00\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 166.4/166.4 kB ? eta 0:00:00\n",
      "Downloading aiohttp-3.9.1-cp310-cp310-win_amd64.whl (364 kB)\n",
      "   ---------------------------------------- 0.0/364.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 364.6/364.6 kB ? eta 0:00:00\n",
      "Downloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "   ---------------------------------------- 0.0/330.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 330.3/330.3 kB ? eta 0:00:00\n",
      "Downloading numpy-1.26.3-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 5.3/15.8 MB 170.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.1/15.8 MB 131.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.6/15.8 MB 131.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 65.6 MB/s eta 0:00:00\n",
      "Downloading pyarrow-14.0.2-cp310-cp310-win_amd64.whl (24.6 MB)\n",
      "   ---------------------------------------- 0.0/24.6 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 4.4/24.6 MB 137.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 7.3/24.6 MB 116.2 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 7.3/24.6 MB 116.2 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 7.3/24.6 MB 116.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 7.8/24.6 MB 35.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 12.9/24.6 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.5/24.6 MB 131.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.6 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.6/24.6 MB 72.5 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp310-cp310-win_amd64.whl (145 kB)\n",
      "   ---------------------------------------- 0.0/145.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 145.3/145.3 kB ? eta 0:00:00\n",
      "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 62.6/62.6 kB ? eta 0:00:00\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB ? eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 134.8/134.8 kB ? eta 0:00:00\n",
      "Downloading pandas-2.1.4-cp310-cp310-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 4.4/10.7 MB 142.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.1/10.7 MB 146.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 93.8 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.8/60.8 kB ? eta 0:00:00\n",
      "Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "   ---------------------------------------- 0.0/162.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 162.5/162.5 kB ? eta 0:00:00\n",
      "Downloading charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl (100 kB)\n",
      "   ---------------------------------------- 0.0/100.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 100.3/100.3 kB ? eta 0:00:00\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.4/50.4 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "   ---------------------------------------- 0.0/61.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 61.6/61.6 kB ? eta 0:00:00\n",
      "Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "   ---------------------------------------- 0.0/502.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 502.5/502.5 kB 32.8 MB/s eta 0:00:00\n",
      "Downloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "   ---------------------------------------- 0.0/346.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 346.6/346.6 kB ? eta 0:00:00\n",
      "Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "   ---------------------------------------- 0.0/104.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 104.6/104.6 kB ? eta 0:00:00\n",
      "Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.4/76.4 kB ? eta 0:00:00\n",
      "Installing collected packages: pytz, xxhash, urllib3, tzdata, tqdm, pyyaml, pyarrow-hotfix, numpy, multidict, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, async-timeout, yarl, requests, pyarrow, pandas, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.9.1 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.2.0 certifi-2023.11.17 charset-normalizer-3.3.2 datasets-2.16.1 dill-0.3.7 filelock-3.13.1 frozenlist-1.4.1 fsspec-2023.10.0 huggingface-hub-0.20.2 idna-3.6 multidict-6.0.4 multiprocess-0.70.15 numpy-1.26.3 pandas-2.1.4 pyarrow-14.0.2 pyarrow-hotfix-0.6 pytz-2023.3.post1 pyyaml-6.0.1 requests-2.31.0 tqdm-4.66.1 tzdata-2023.4 urllib3-2.1.0 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-win_amd64.whl (977 kB)\n",
      "     ---------------------------------------- 0.0/977.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/977.5 kB ? eta -:--:--\n",
      "     -- ------------------------------------ 61.4/977.5 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------  972.8/977.5 kB 8.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 977.5/977.5 kB 8.9 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n",
      "Collecting torch\n",
      "  Downloading torch-2.1.2-cp310-cp310-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from torch) (4.9.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "     ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.1/5.7 MB 1.7 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 1.0/5.7 MB 12.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 5.4/5.7 MB 48.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.7/5.7 MB 45.9 MB/s eta 0:00:00\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "     ---------------------------------------- 0.0/133.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 133.1/133.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-2.1.3-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 536.2/536.2 kB ? eta 0:00:00\n",
      "Downloading torch-2.1.2-cp310-cp310-win_amd64.whl (192.3 MB)\n",
      "   ---------------------------------------- 0.0/192.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 4.3/192.3 MB 139.7 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 8.5/192.3 MB 109.2 MB/s eta 0:00:02\n",
      "   -- ------------------------------------ 13.8/192.3 MB 108.8 MB/s eta 0:00:02\n",
      "   -- ------------------------------------ 14.7/192.3 MB 108.8 MB/s eta 0:00:02\n",
      "   -- ------------------------------------ 14.7/192.3 MB 108.8 MB/s eta 0:00:02\n",
      "   -- ------------------------------------ 14.7/192.3 MB 108.8 MB/s eta 0:00:02\n",
      "   -- ------------------------------------ 14.7/192.3 MB 108.8 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 19.8/192.3 MB 40.9 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 23.8/192.3 MB 38.5 MB/s eta 0:00:05\n",
      "   ----- --------------------------------- 29.4/192.3 MB 131.2 MB/s eta 0:00:02\n",
      "   ------ -------------------------------- 34.4/192.3 MB 110.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------ 40.1/192.3 MB 108.8 MB/s eta 0:00:02\n",
      "   --------- ----------------------------- 45.7/192.3 MB 108.8 MB/s eta 0:00:02\n",
      "   ---------- ---------------------------- 50.2/192.3 MB 110.0 MB/s eta 0:00:02\n",
      "   ----------- --------------------------- 55.2/192.3 MB 108.8 MB/s eta 0:00:02\n",
      "   ------------ -------------------------- 59.4/192.3 MB 108.8 MB/s eta 0:00:02\n",
      "   ------------- ------------------------- 64.7/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------ 70.3/192.3 MB 108.8 MB/s eta 0:00:02\n",
      "   --------------- ----------------------- 74.4/192.3 MB 108.8 MB/s eta 0:00:02\n",
      "   ---------------- ---------------------- 79.7/192.3 MB 108.8 MB/s eta 0:00:02\n",
      "   ---------------- ---------------------- 83.8/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   ------------------ -------------------- 89.2/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 93.5/192.3 MB 93.0 MB/s eta 0:00:02\n",
      "   ------------------- ------------------- 97.8/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "   -------------------- ----------------- 103.3/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "   --------------------- ---------------- 107.4/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------------- --------------- 112.6/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   ----------------------- -------------- 117.9/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "   ------------------------ ------------- 122.0/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "   ------------------------- ------------ 127.6/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "   -------------------------- ----------- 133.3/192.3 MB 110.0 MB/s eta 0:00:01\n",
      "   --------------------------- ----------- 137.3/192.3 MB 93.9 MB/s eta 0:00:01\n",
      "   ---------------------------- --------- 142.9/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   ----------------------------- -------- 148.2/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "   ------------------------------ ------- 153.6/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "   ------------------------------- ------ 158.0/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ----- 163.5/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 167.5/192.3 MB 93.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- --- 173.1/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- -- 177.2/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ - 182.5/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "   -------------------------------------  187.8/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "   -------------------------------------  192.2/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 93.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 93.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 93.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 93.0 MB/s eta 0:00:01\n",
      "   --------------------------------------- 192.3/192.3 MB 32.7 MB/s eta 0:00:00\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 102.3 MB/s eta 0:00:00\n",
      "Downloading MarkupSafe-2.1.3-cp310-cp310-win_amd64.whl (17 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, jinja2, torch\n",
      "Successfully installed MarkupSafe-2.1.3 jinja2-3.1.2 mpmath-1.3.0 networkx-3.2.1 sympy-1.12 torch-2.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.12.25-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ----------------------------- ---------- 30.7/42.0 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 42.0/42.0 kB 511.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp310-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.1-cp310-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Using cached transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "Downloading regex-2023.12.25-cp310-cp310-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.5 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 92.2/269.5 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 269.5/269.5 kB 4.2 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp310-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 277.3/277.3 kB 16.7 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 70.6 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2023.12.25 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.36.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Hugging face에서 데이터 \n",
    "from datasets import load_dataset\n",
    "nsmc = load_dataset('nsmc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 150000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 확인해 보기\n",
    "nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data, test_data 분리 (학습 시간 상 2000개만 샘플링)\n",
    "train_data = nsmc['train'].shuffle(seed=42).select(range(2000))\n",
    "test_data = nsmc['test'].shuffle(seed=42).select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document', 'label'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train, test 데이터 확인해 보기\n",
    "train_data\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 625/625 [00:00<?, ?B/s] \n",
      "model.safetensors: 100%|██████████| 714M/714M [00:07<00:00, 96.2MB/s] \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "tokenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|██████████| 996k/996k [00:00<00:00, 1.25MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.96M/1.96M [00:00<00:00, 2.00MB/s]\n"
     ]
    }
   ],
   "source": [
    "# 감성분석 'bert-base-multilingual-cased' 모델 로드\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PreTrainedTokenizerFast.tokenize() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\NLP_day1\\NLP_nltk_pj\\day1\\6_bert_classification.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/NLP_day1/NLP_nltk_pj/day1/6_bert_classification.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_data[\u001b[39m'\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/NLP_day1/NLP_nltk_pj/day1/6_bert_classification.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tokenizer\u001b[39m.\u001b[39;49mtokenize()\n",
      "\u001b[1;31mTypeError\u001b[0m: PreTrainedTokenizerFast.tokenize() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "# train_data['document'][0]\n",
    "# tokenizer.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 11399, 12225, 119, 9788, 9435, 10739, 71439, 11467, 9485, 38709, 70146, 9788, 9435, 10739, 71439, 11467, 8977, 33305, 11903, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.tokenize 테스트\n",
    "tokenizer(train_data['document'][0])\n",
    "\n",
    "#TOKEN TYPE IDS : 각 토큰의 문장 IDD\n",
    "#attention_mask : padding 값 : 1, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data encoding (tokenizing)\n",
    "# train_encoding\n",
    "train_encoding = tokenizer(\n",
    "    train_data['document'],\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "# test_encoding\n",
    "test_encoding = tokenizer(\n",
    "    test_data['document'],\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2000\n"
     ]
    }
   ],
   "source": [
    "# Encoding된 데이터, 개수 확인하기\n",
    "train_encoding\n",
    "print(len(train_encoding['input_ids']), len(test_encoding['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special token ids : [100, 102, 0, 101, 103]\n",
      "special token  : ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n"
     ]
    }
   ],
   "source": [
    "# BERT Special token\n",
    "print(f\"special token ids : {tokenizer.all_special_ids}\")\n",
    "print(f\"special token  : {tokenizer.all_special_tokens}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Bert fine-tuning: 학습 데이터 만들기]\n",
    "1. input_ids\n",
    "2. token_type_ids\n",
    "3. attention_mask  \n",
    "----------------------> tokenizer를 활용하여 손쉽게 구성 완료\n",
    "4. labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 class 구성 (학습 sequence와, label 분류)\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NSMCDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        data['labels'] = torch.tensor(self.labels[idx]).long()\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class 실행\n",
    "train_set = NSMCDataset(train_encoding, train_data['label'])\n",
    "test_set = NSMCDataset(test_encoding, train_data['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101, 11399, 12225,   119,  9788,  9435, 10739, 71439, 11467,  9485,\n",
       "         38709, 70146,  9788,  9435, 10739, 71439, 11467,  8977, 33305, 11903,\n",
       "           119,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(1)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train셋 확인\n",
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101, 14796, 27728, 10230,   106,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(1)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test셋 확인\n",
    "test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from accelerate) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from accelerate) (0.20.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from accelerate) (0.4.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (4.36.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\NLP_day1\\NLP_nltk_pj\\day1\\6_bert_classification.ipynb Cell 23\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/NLP_day1/NLP_nltk_pj/day1/6_bert_classification.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Trainer 클래스로 학습하기\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/NLP_day1/NLP_nltk_pj/day1/6_bert_classification.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m TrainingArguments, Trainer\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/NLP_day1/NLP_nltk_pj/day1/6_bert_classification.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/NLP_day1/NLP_nltk_pj/day1/6_bert_classification.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     output_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./outputs\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/NLP_day1/NLP_nltk_pj/day1/6_bert_classification.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     logging_dir \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m./logs\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/NLP_day1/NLP_nltk_pj/day1/6_bert_classification.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/NLP_day1/NLP_nltk_pj/day1/6_bert_classification.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/NLP_day1/NLP_nltk_pj/day1/6_bert_classification.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     per_device_eval_batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/NLP_day1/NLP_nltk_pj/day1/6_bert_classification.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     logging_steps\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/NLP_day1/NLP_nltk_pj/day1/6_bert_classification.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m#50번째마다 로깅 데이터를 찍겠다\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/NLP_day1/NLP_nltk_pj/day1/6_bert_classification.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     save_steps\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/NLP_day1/NLP_nltk_pj/day1/6_bert_classification.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     save_total_limit\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/NLP_day1/NLP_nltk_pj/day1/6_bert_classification.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n",
      "File \u001b[1;32m<string>:121\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha)\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\venv\\lib\\site-packages\\transformers\\training_args.py:1493\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1487\u001b[0m     \u001b[39mif\u001b[39;00m version\u001b[39m.\u001b[39mparse(version\u001b[39m.\u001b[39mparse(torch\u001b[39m.\u001b[39m__version__)\u001b[39m.\u001b[39mbase_version) \u001b[39m==\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m2.0.0\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16:\n\u001b[0;32m   1488\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1490\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1491\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1492\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[1;32m-> 1493\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1494\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1495\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1496\u001b[0m     \u001b[39mand\u001b[39;00m (get_xla_device_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1497\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16_full_eval)\n\u001b[0;32m   1498\u001b[0m ):\n\u001b[0;32m   1499\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1500\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1501\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1502\u001b[0m     )\n\u001b[0;32m   1504\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1505\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1506\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16_full_eval)\n\u001b[0;32m   1514\u001b[0m ):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\venv\\lib\\site-packages\\transformers\\training_args.py:1941\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1937\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1938\u001b[0m \u001b[39mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   1939\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m requires_backends(\u001b[39mself\u001b[39m, [\u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m-> 1941\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_devices\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\venv\\lib\\site-packages\\transformers\\utils\\generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     52\u001b[0m cached \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, attr, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m cached \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     cached \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfget(obj)\n\u001b[0;32m     55\u001b[0m     \u001b[39msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\venv\\lib\\site-packages\\transformers\\training_args.py:1841\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   1840\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_accelerate_available(min_version\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m0.20.1\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1841\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m   1842\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1843\u001b[0m         )\n\u001b[0;32m   1844\u001b[0m     AcceleratorState\u001b[39m.\u001b[39m_reset_state(reset_partial_state\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1845\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistributed_state \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "# Trainer 클래스로 학습하기\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./outputs',\n",
    "    logging_dir = './logs',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=50,\n",
    "    #50번째마다 로깅 데이터를 찍겠다\n",
    "    save_steps=50,\n",
    "    save_total_limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습을 위해 gpu 사용 확인 -> gpu 가능: 'cuda' , gpu 불가능 : 'cpu'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "#가상환경이라 ...시ㅡ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets의 load_metric을 활용하여 성능 측정\n",
    "from datasets import load_metric\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    matrix_1 = load_metric('accuracy')\n",
    "    matrix_2 = load_metric('f1')\n",
    "\n",
    "    acc = matrix_1.compute(predictions = preds, references=labels)['accuracy']\n",
    "    f1 = matrix_2.compute(predictions = preds, references=labels)['f1']\n",
    "\n",
    "    \n",
    "\n",
    "    return {f\"accuracy\" : acc, \"f1\" : f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model GPU 사용\n",
    "model.to(device)\n",
    "\n",
    "# Trainer 구성\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_set,\n",
    "    eval_dataset= test_set,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습하기\n",
    "trainer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 약 25분 소요\n",
    "#{'loss': 0.6646, 'learning_rate': 4.603174603174603e-05, 'epoch': 0.79}\n",
    "# {'loss': 0.5363, 'learning_rate': 4.2063492063492065e-05, 'epoch': 1.59}\n",
    "# {'loss': 0.3994, 'learning_rate': 3.809523809523809e-05, 'epoch': 2.38}\n",
    "# {'loss': 0.3099, 'learning_rate': 3.412698412698413e-05, 'epoch': 3.17}\n",
    "# {'loss': 0.188, 'learning_rate': 3.0158730158730158e-05, 'epoch': 3.97}\n",
    "# {'loss': 0.1881, 'learning_rate': 2.6190476190476192e-05, 'epoch': 4.76}\n",
    "# {'loss': 0.1174, 'learning_rate': 2.2222222222222223e-05, 'epoch': 5.56}\n",
    "# {'loss': 0.0919, 'learning_rate': 1.8253968253968254e-05, 'epoch': 6.35}\n",
    "# {'loss': 0.0817, 'learning_rate': 1.4285714285714285e-05, 'epoch': 7.14}\n",
    "# {'loss': 0.0565, 'learning_rate': 1.0317460317460318e-05, 'epoch': 7.94}\n",
    "# {'loss': 0.0391, 'learning_rate': 6.349206349206349e-06, 'epoch': 8.73}\n",
    "# {'loss': 0.0426, 'learning_rate': 2.3809523809523808e-06, 'epoch': 9.52}\n",
    "# {'train_runtime': 1618.84, 'train_samples_per_second': 12.355, 'train_steps_per_second': 0.389, 'train_loss': 0.2167289758485461, 'epoch': 10.0}\n",
    "# TrainOutput(global_step=630, training_loss=0.2167289758485461, metrics={'train_runtime': 1618.84, 'train_samples_per_second': 12.355, 'train_steps_per_second': 0.389, 'train_loss': 0.2167289758485461, 'epoch': 10.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'eval_loss': 1.198122501373291,\n",
    "#  'eval_accuracy': 0.766,\n",
    "#  'eval_f1': 0.7662337662337662,\n",
    "#  'eval_runtime': 46.6907,\n",
    "#  'eval_samples_per_second': 42.835,\n",
    "#  'eval_steps_per_second': 1.349,\n",
    "#  'epoch': 10.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install ipywidgets\n",
    "# ! conda install -c conda-forge ipympl\n",
    "# !jupyter labextension list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch를 활용한 네이티브 방식으로 학습하기  \n",
    ": TrainingArguments와 Trainer를 활용하지 않고 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_metric\n",
    "\n",
    "# train 메소드\n",
    "def train(epoch, model, dataloader, optimizer, device):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate 메소드\n",
    "def evaluate(model, dataloader, device):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL | LOSS: 1.2046 ACC: 0.7710 F1: 0.7644"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "305494ec69d5ad97a583cc76e8fd52e450123bc765c435a27726a289dbe2d5e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
